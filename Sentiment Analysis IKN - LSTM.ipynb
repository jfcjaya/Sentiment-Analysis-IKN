{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis IKN - LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A. Libraries"
      ],
      "metadata": {
        "id": "Z_FoOXF5wSBJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jiUBjQWwIDr",
        "outputId": "130652c4-cf25-4e1b-adb2-24c021a0acef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.8.3-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (2.23.0)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.9.1\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.3.2.post1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.24.3)\n",
            "Installing collected packages: beautifulsoup4, deep-translator\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.11.1 deep-translator-1.8.3\n",
            "--2022-04-30 13:39:06--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://setup.johnsnowlabs.com/colab.sh [following]\n",
            "--2022-04-30 13:39:06--  https://setup.johnsnowlabs.com/colab.sh\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2022-04-30 13:39:07--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1453 (1.4K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-30 13:39:08 (22.7 MB/s) - written to stdout [1453/1453]\n",
            "\n",
            "setup Colab for PySpark 3.0.3 and Spark NLP 3.4.3\n",
            "Installing PySpark 3.0.3 and Spark NLP 3.4.3\n",
            "\u001b[K     |████████████████████████████████| 209.1 MB 50 kB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 39.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 44.3 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark\n",
        "!pip install nltk\n",
        "!pip install -U deep-translator\n",
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import findspark\n",
        "import keras\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "import sparknlp\n",
        "import string\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, f1_score, recall_score\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *\n",
        "from sparknlp.pretrained import ResourceDownloader\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "hwE4_YjywyoG"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-r7GLGDxhTZ",
        "outputId": "fe76ee1d-14e7-4d36-ba40-1e3e6686078b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B. Dataset"
      ],
      "metadata": {
        "id": "08fN7rz_1sn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column = ['tweet', 'sentiment']"
      ],
      "metadata": {
        "id": "GcMzO6e2smj6"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(link):\n",
        "    # read file from given link\n",
        "    temp = pd.read_json(link, orient = \"index\")\n",
        "\n",
        "    # remove unnecessary column\n",
        "    temp = temp[column]\n",
        "\n",
        "    # drop rows with null values\n",
        "    temp = temp.dropna()\n",
        "\n",
        "    # typecast sentiment as int (by default it is float)\n",
        "    temp['sentiment'] = temp['sentiment'].astype(int)\n",
        "\n",
        "    # select only data with sentiment = 1 or -1\n",
        "    temp = temp.loc[(temp['sentiment'] == 1) | (temp['sentiment'] == -1)]\n",
        "    temp.reset_index(drop = True, inplace = True)\n",
        "\n",
        "    return temp"
      ],
      "metadata": {
        "id": "AjefcM3jYkyd"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = get_data(\"https://raw.githubusercontent.com/VinsenN/Sentiment-Analysis-Pemindahan-Ibu-Kota/main/data/IKN.json\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "1bc05881-c943-4f05-c086-9718e7d57ca9",
        "id": "VZRhQc5Xsmj7"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               tweet  sentiment\n",
              "0  @jokowi saya sangat setuju pak, bahkan lebih s...          1\n",
              "1  @hnurwahid @FPKSDPRRI Saya setuju ibu kota pin...          1\n",
              "2  @MardaniAliSera @FPKSDPRRI Saya dan mayoritas ...          1\n",
              "3  cocok ibu kota pindah ke kalimantan apalagi gu...          1\n",
              "4  @geedeulbeyou1 Jadi, kepada lo yang gak setuju...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4789fcb9-9721-4f75-9856-ae9f1b48bdbb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@jokowi saya sangat setuju pak, bahkan lebih s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@hnurwahid @FPKSDPRRI Saya setuju ibu kota pin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@MardaniAliSera @FPKSDPRRI Saya dan mayoritas ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cocok ibu kota pindah ke kalimantan apalagi gu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@geedeulbeyou1 Jadi, kepada lo yang gak setuju...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4789fcb9-9721-4f75-9856-ae9f1b48bdbb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4789fcb9-9721-4f75-9856-ae9f1b48bdbb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4789fcb9-9721-4f75-9856-ae9f1b48bdbb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (20, 15))\n",
        "df.groupby('sentiment').count().plot.bar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "e4b03f6c-17fc-492a-dc04-349a90d87199",
        "id": "spL4DmxAsmj8"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f30f2ccc490>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1080 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEGCAYAAACevtWaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATaklEQVR4nO3df7BfdZ3f8eerEKVKOiC5pEjAID+ske1G9oqhNAyUlV+7K8sOsmaqy682OEJd3e20rHbWbYsrdpcfQ6s4ccyAIyC06ApdSkFkV9aCuwmb8iPoGizUG2O4JKCg/JDw7h/3xH6NN94f3+/N5X7yfMx853vO+5zz/bwvc/Oaw+ee7zmpKiRJbfl7s92AJGnwDHdJapDhLkkNMtwlqUGGuyQ1aM/ZbgBgwYIFtXjx4tluQ5LmlLVr1z5ZVUPjbXtFhPvixYtZs2bNbLchSXNKksd3ts1pGUlqkOEuSQ0y3CWpQa+IOXdJmshPfvITRkZGeP7552e7lV1ur732YtGiRcybN2/SxxjukuaEkZER5s+fz+LFi0ky2+3sMlXFli1bGBkZ4ZBDDpn0cU7LSJoTnn/+efbbb7/dKtgBkrDffvtN+f9YJgz3JAcluTvJ+iQPJ/ndrv66JHcm+Xb3vm9XT5KrkmxI8kCSo6b1E0nSDna3YN9uOj/3ZM7cXwJ+v6qWAMuAC5MsAS4G7qqqw4G7unWAU4HDu9dK4OopdyVJ6suEc+5VtQnY1C0/k+QR4EDgdOD4brdrgb8A/m1X/1yN3Sj+viT7JDmg+xxJGojFF//5QD/vsUt/7Rduf/rpp7n++ut5//vfP9Bxt7vyyitZuXIlr3nNawbyeVP6g2qSxcBbgW8AC3sC+/vAwm75QOC7PYeNdLWfCfckKxk7s+fggw+eYtuzY9C/TLu7if4xSa8kTz/9NJ/61KdmNNzf8573DCzcJ/0H1SR7AzcDH6yqH/Zu687Sp/RIp6paVVXDVTU8NDTurREk6RXj4osv5tFHH2Xp0qWce+653HLLLQCcccYZnHfeeQCsXr2aj3zkIwB8/vOf5+ijj2bp0qVccMEFbNu2DYA77riDY445hqOOOop3vetdPPvss1x11VV873vf44QTTuCEE04YSL+TCvck8xgL9uuq6otdeXOSA7rtBwBPdPWNwEE9hy/qapI0Z1166aUceuihrFu3jpNPPpl77rkHgI0bN7J+/XoA7rnnHo477jgeeeQRbrzxRr7+9a+zbt069thjD6677jqefPJJLrnkEr7yla9w//33Mzw8zOWXX84HPvABXv/613P33Xdz9913D6TfCadlMvZn2s8Cj1TV5T2bbgHOBi7t3r/cU78oyReAtwM/cL5dUkuWL1/OlVdeyfr161myZAlPPfUUmzZt4t577+Wqq67i2muvZe3atbztbW8D4LnnnmP//ffnvvvuY/369Rx77LEAvPjiixxzzDEz0uNk5tyPBd4LPJhkXVf7MGOhflOS84HHgbO6bbcBpwEbgB8D5w60Y0maZQceeCBPP/00t99+O8cddxxbt27lpptuYu+992b+/PlUFWeffTYf//jHf+a4W2+9lXe84x3ccMMNM97jZK6W+StgZxdZnjjO/gVc2GdfkvSKMn/+fJ555pmfri9btowrr7ySr371q2zZsoUzzzyTM888E4ATTzyR008/nQ996EPsv//+bN26lWeeeYZly5Zx4YUXsmHDBg477DB+9KMfsXHjRo444oiffv6CBQsG0q+3H5A0J+3qq632228/jj32WI488khOPfVUli9fzh133MFhhx3GG97wBrZu3cry5csBWLJkCZdccgknnXQSL7/8MvPmzeOTn/wky5Yt45prrmHFihW88MILAFxyySUcccQRrFy5klNOOeWnc+/9ytiJ9uwaHh6uufCwDi+FHCwvhdRUPPLII7z5zW+e7TZmzXg/f5K1VTU83v7eW0aSGmS4S1KDDHdJc8YrYRp5Nkzn5zbcJc0Je+21F1u2bNntAn77/dz32muvKR3n1TKS5oRFixYxMjLC6OjobLeyy21/EtNUGO6S5oR58+ZN6UlEuzunZSSpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNmjDck6xO8kSSh3pqNyZZ170e2/6EpiSLkzzXs+3TM9m8JGl8k/mG6jXAfwE+t71QVb+9fTnJZcAPevZ/tKqWDqpBSdLUTeYxe19Lsni8bd3Ds88C/tlg25Ik9aPfOfflwOaq+nZP7ZAkf5vkL5Ms7/PzJUnT0O+Nw1YAvY/x3gQcXFVbkvwK8GdJ3lJVP9zxwCQrgZUABx98cJ9tSJJ6TfvMPcmewG8BN26vVdULVbWlW14LPAocMd7xVbWqqoaranhoaGi6bUiSxtHPtMyvAt+sqpHthSRDSfbolt8IHA58p78WJUlTNZlLIW8A7gXelGQkyfndpnfzs1MyAMcBD3SXRv434H1VtXWQDUuSJjaZq2VW7KR+zji1m4Gb+29LktQPv6EqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDZrMY/ZWJ3kiyUM9tT9KsjHJuu51Ws+2P0iyIcm3kpw8U41LknZuMmfu1wCnjFO/oqqWdq/bAJIsYezZqm/pjvnU9gdmS5J2nQnDvaq+Bkz2IdenA1+oqheq6v8AG4Cj++hPkjQN/cy5X5TkgW7aZt+udiDw3Z59Rrraz0myMsmaJGtGR0f7aEOStKPphvvVwKHAUmATcNlUP6CqVlXVcFUNDw0NTbMNSdJ4phXuVbW5qrZV1cvAZ/j/Uy8bgYN6dl3U1SRJu9C0wj3JAT2rZwDbr6S5BXh3klcnOQQ4HPjr/lqUJE3VnhPtkOQG4HhgQZIR4KPA8UmWAgU8BlwAUFUPJ7kJWA+8BFxYVdtmpnVJ0s5MGO5VtWKc8md/wf4fAz7WT1OSpP74DVVJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNWjCG4dJmhsWX/zns91CMx679Ndmu4W+eeYuSQ0y3CWpQYa7JDXIcJekBk0Y7klWJ3kiyUM9tT9J8s0kDyT5UpJ9uvriJM8lWde9Pj2TzUuSxjeZM/drgFN2qN0JHFlV/xj4O+APerY9WlVLu9f7BtOmJGkqJgz3qvoasHWH2h1V9VK3eh+waAZ6kyRN0yDm3M8D/kfP+iFJ/jbJXyZZvrODkqxMsibJmtHR0QG0IUnarq9wT/IR4CXguq60CTi4qt4K/B5wfZJ/MN6xVbWqqoaranhoaKifNiRJO5h2uCc5B/h14J9XVQFU1QtVtaVbXgs8ChwxgD4lSVMwrXBPcgrwb4B3VtWPe+pDSfbolt8IHA58ZxCNSpImb8J7yyS5ATgeWJBkBPgoY1fHvBq4MwnAfd2VMccB/yHJT4CXgfdV1dZxP1iSNGMmDPeqWjFO+bM72fdm4OZ+m5Ik9cdvqEpSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDJhXuSVYneSLJQz211yW5M8m3u/d9u3qSXJVkQ5IHkhw1U81LksY32TP3a4BTdqhdDNxVVYcDd3XrAKcy9mDsw4GVwNX9tylJmopJhXtVfQ3Y8UHXpwPXdsvXAr/ZU/9cjbkP2CfJAYNoVpI0Of3MuS+sqk3d8veBhd3ygcB3e/Yb6Wo/I8nKJGuSrBkdHe2jDUnSjgbyB9WqKqCmeMyqqhququGhoaFBtCFJ6vQT7pu3T7d070909Y3AQT37LepqkqRdpJ9wvwU4u1s+G/hyT/13uqtmlgE/6Jm+kSTtAntOZqckNwDHAwuSjAAfBS4FbkpyPvA4cFa3+23AacAG4MfAuQPuWZI0gUmFe1Wt2MmmE8fZt4AL+2lKktQfv6EqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDZrUk5jGk+RNwI09pTcCfwjsA/xLYLSrf7iqbpt2h5KkKZt2uFfVt4ClAEn2ADYCX2LsmalXVNWfDqRDSdKUDWpa5kTg0ap6fECfJ0nqw6DC/d3ADT3rFyV5IMnqJPuOd0CSlUnWJFkzOjo63i6SpGnqO9yTvAp4J/Bfu9LVwKGMTdlsAi4b77iqWlVVw1U1PDQ01G8bkqQegzhzPxW4v6o2A1TV5qraVlUvA58Bjh7AGJKkKRhEuK+gZ0omyQE9284AHhrAGJKkKZj21TIASV4LvAO4oKf8n5IsBQp4bIdtkqRdoK9wr6ofAfvtUHtvXx1JkvrmN1QlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQX09iQkgyWPAM8A24KWqGk7yOuBGYDFjj9o7q6qe6ncsSdLkDOrM/YSqWlpVw936xcBdVXU4cFe3LknaRWZqWuZ04Npu+VrgN2doHEnSOAYR7gXckWRtkpVdbWFVbeqWvw8s3PGgJCuTrEmyZnR0dABtSJK263vOHfinVbUxyf7AnUm+2buxqipJ7XhQVa0CVgEMDw//3HZJ0vT1feZeVRu79yeALwFHA5uTHADQvT/R7ziSpMnrK9yTvDbJ/O3LwEnAQ8AtwNndbmcDX+5nHEnS1PQ7LbMQ+FKS7Z91fVXdnuRvgJuSnA88DpzV5ziSpCnoK9yr6jvAL49T3wKc2M9nS5Kmz2+oSlKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoOmHe5JDkpyd5L1SR5O8rtd/Y+SbEyyrnudNrh2JUmT0c9j9l4Cfr+q7u8ekr02yZ3dtiuq6k/7b0+SNB3TDveq2gRs6pafSfIIcOCgGpMkTd9A5tyTLAbeCnyjK12U5IEkq5Psu5NjViZZk2TN6OjoINqQJHX6DvckewM3Ax+sqh8CVwOHAksZO7O/bLzjqmpVVQ1X1fDQ0FC/bUiSevQV7knmMRbs11XVFwGqanNVbauql4HPAEf336YkaSr6uVomwGeBR6rq8p76AT27nQE8NP32JEnT0c/VMscC7wUeTLKuq30YWJFkKVDAY8AFfXUoSZqyfq6W+Ssg42y6bfrtSJIGwW+oSlKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoNmLNyTnJLkW0k2JLl4psaRJP28GQn3JHsAnwROBZYw9lzVJTMxliTp583UmfvRwIaq+k5VvQh8ATh9hsaSJO1g2g/InsCBwHd71keAt/fukGQlsLJbfTbJt2aol93RAuDJ2W5iIvnEbHegWeDv5mC9YWcbZircJ1RVq4BVszV+y5Ksqarh2e5D2pG/m7vOTE3LbAQO6llf1NUkSbvATIX73wCHJzkkyauAdwO3zNBYkqQdzMi0TFW9lOQi4H8CewCrq+rhmRhL43K6S69U/m7uIqmq2e5BkjRgfkNVkhpkuEtSgwx3SWqQ4S5JDTLcG5Zk79nuQdqZJOfOdg8t82qZhiX5v1V18Gz3IY3H38+ZNWu3H9BgJPm9nW0CPHPXrErywM42AQt3ZS+7G8N97vtj4E+Al8bZ5rSbZttC4GTgqR3qAf7Xrm9n92G4z333A39WVWt33JDkX8xCP1Kv/w7sXVXrdtyQ5C92fTu7D+fc57gkbwK2VNWTPbV/WFXfT7KwqjbPYnuSZonh3qAk91fVUbPdh6TZ45xsmzLbDUiaXYZ7mz4z2w1Iml1Oy0hSgzxzl6QGGe6S1CDDXbu9JEuTnNaz/s4kF8/wmMcn+SczOYZ2b4a7BEuBn4Z7Vd1SVZfO8JjHA4a7Zox/UNWcluS1wE3AIsae1/sfgQ3A5YzdW+dJ4Jyq2tR9I/IbwAnAPsD53foG4O8DG4GPd8vDVXVRkmuA54C3AvsD5wG/AxwDfKOqzun6OAn498CrgUeBc6vq2SSPAdcCvwHMA94FPA/cB2wDRoF/VVX3zMR/H+2+PHPXXHcK8L2q+uWqOhK4HfjPwJlV9SvAauBjPfvvWVVHAx8EPlpVLwJ/CNxYVUur6sZxxtiXsTD/EHALcAXwFuCXuimdBcC/A361+/LYGqD3hm5PdvWrgX9dVY8Bnwau6MY02DVw3ltGc92DwGVJPsHYfUyeAo4E7kwCY2fzm3r2/2L3vhZYPMkxbq2qSvIgsLmqHgRI8nD3GYuAJcDXuzFfBdy7kzF/awo/mzRthrvmtKr6uyRHMTZnfgnwVeDhqjpmJ4e80L1vY/K//9uPeblnefv6nt1n3VlVKwY4ptQXp2U0pyV5PfDjqvo8Y7c+fjswlOSYbvu8JG+Z4GOeAeb30cZ9wLFJDuvGfG2SI2Z4TOkXMtw11/0S8NdJ1gEfZWz+/EzgE0n+N7COia9KuRtYkmRdkt+eagNVNQqcA9zQPZziXuAfTXDYrcAZ3ZjLpzqmNBGvlpGkBnnmLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg/4fkjNMdiKCqV0AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C. Preprocess - Basic"
      ],
      "metadata": {
        "id": "oCFj4rjJ3Xxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# harga     emas      -> harga emas\n",
        "def remove_redundant_space(text):\n",
        "    # trailing spaces\n",
        "    text = re.sub(\" +$\", \"\", text)\n",
        "    # leading spaces\n",
        "    text = re.sub(\"^ +\", \"\", text)\n",
        "    # redundant spaces\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "a7zCepPk3pel"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_mention(text):\n",
        "    return re.sub(\"@\\S+\", \" \", text)"
      ],
      "metadata": {
        "id": "UDq4qBfr3xY5"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_hashtag(text):\n",
        "    return re.sub(\"#\\S+\", \" \", text)"
      ],
      "metadata": {
        "id": "mWW2HwLu3z5c"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# emojis, dll\n",
        "def remove_special_char(text):\n",
        "    return re.sub(r\"[^a-zA-z0-9.,!?/:;\\\"\\'\\s]\" , \" \", text)"
      ],
      "metadata": {
        "id": "qDnzdkAT31E3"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_num(text):\n",
        "    return re.sub(r\"\\d+\", \" \", text)"
      ],
      "metadata": {
        "id": "7vAEpkuN32jq"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hello! -> hello\n",
        "def remove_punctuation(text):\n",
        "    text = re.sub(\"_\", \" \", text)\n",
        "    text = re.sub(\"_+\", \" \", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
        "    return text;"
      ],
      "metadata": {
        "id": "0muLlEo834OC"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_basic(text):\n",
        "    text = remove_mention(text)\n",
        "    text = remove_hashtag(text)\n",
        "    text = remove_special_char(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_num(text)\n",
        "    text = text.lower()\n",
        "    text = remove_redundant_space(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "fHoUr3Fm4gJA"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_text = \"       !\\\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~ HALO   🤣   Halo123 Halo     123 halo 😪😪😪😪 (Test) #IKN #Ibu_Kota_Negara @Vinsen_Nawir @Emmanuel_Henry     \"\n",
        "print(preprocess_basic(testing_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxRnHdmP4dLi",
        "outputId": "ba55cf5d-9a3c-410f-ac70-15c6ef6aaaff"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "halo halo halo halo test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# D. Preprocess - More"
      ],
      "metadata": {
        "id": "KmuwEG3q55Vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove insignificant words"
      ],
      "metadata": {
        "id": "I4voNkQNJN3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define insignificant words as the word that has less occurence than a predefined threshold."
      ],
      "metadata": {
        "id": "BTuFXgxAJS35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_noise(text: str, \n",
        "                 freq: Counter, \n",
        "                 threshold: int):\n",
        "    \n",
        "    text = \" \".join([word for word in text.split()\n",
        "                     if freq[word] >= threshold])\n",
        "    return remove_redundant_space(text)"
      ],
      "metadata": {
        "id": "lomUNQ_cJL6a"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove slangs and abbreviations"
      ],
      "metadata": {
        "id": "XkwDhsB95-Us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict1 = pd.read_csv(\"https://raw.githubusercontent.com/okkyibrohim/id-multi-label-hate-speech-and-abusive-language-detection/master/new_kamusalay.csv\", encoding = \"ISO-8859-1\")\n",
        "dict2 = pd.read_csv(\"https://raw.githubusercontent.com/nasalsabila/kamus-alay/master/colloquial-indonesian-lexicon.csv\")\n",
        "dict3 = pd.read_csv(\"https://raw.githubusercontent.com/VinsenN/Sentiment-Analysis-Pemindahan-Ibu-Kota/main/data/slang/custom-slang.csv\")"
      ],
      "metadata": {
        "id": "f5VNmCgQ59bS"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['slang', 'formal']\n",
        "dict1.columns = columns\n",
        "dict2 = dict2[columns]\n",
        "dict3.columns = columns\n",
        "\n",
        "# Combine all dictionaries\n",
        "dict1 = dict1.append(dict2)\n",
        "dict1 = dict1.append(dict3)\n",
        "\n",
        "# Remove duplicate values\n",
        "dict1.sort_values('slang', inplace = True)\n",
        "dict1.drop_duplicates(subset = columns, keep = False, inplace = True)\n",
        "\n",
        "# Convert into python dict\n",
        "kata_gaul = pd.Series(dict1['formal'].values, index = dict1['slang']).to_dict()"
      ],
      "metadata": {
        "id": "Jcyo6mnf6z3D"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_slang(text):\n",
        "    return \" \".join([kata_gaul[word] if word in kata_gaul\n",
        "                     else word\n",
        "                     for word in text.split(' ')])"
      ],
      "metadata": {
        "id": "3uuatlGC7N9B"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove stopwords"
      ],
      "metadata": {
        "id": "AYjs1xM17h1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating stopword dictionary\n",
        "stopword_list = stopwords.words('indonesian')\n",
        "whitelist = ['tidak', 'bukan', 'enggak', 'daripada', 'melainkan']\n",
        "blacklist = ['jokowi', 'joko', 'widodo', 'ahok', 'rocky', 'gerung', 'pks', 'partai', 'keadilan', 'sejahtera', 'oligarki', 'kalimantan', 'jawa', 'jakarta', 'jkt', 'papua']\n",
        "\n",
        "for word in whitelist:\n",
        "    stopword_list.remove(word)\n",
        "\n",
        "for word in blacklist:\n",
        "    stopword_list.append(word)"
      ],
      "metadata": {
        "id": "zux_7fPZ7rWx"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopword(text):\n",
        "    text = ' '.join([word if word not in stopword_list\n",
        "                     else \"\"\n",
        "                     for word in text.split(' ')])\n",
        "    return remove_redundant_space(text)"
      ],
      "metadata": {
        "id": "kl9ARnWx74Fo"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_text = \"sebagai presiden indonesia saya tidak setuju dengan pernyataan anda\"\n",
        "print(remove_stopword(testing_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iVgTvOI9ER1",
        "outputId": "ca8b0dbb-cf5c-4bad-b3d0-4c1baece41ac"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "presiden indonesia tidak setuju pernyataan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "BI1cSz0U8FOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.init()\n",
        "spark = sparknlp.start()\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbEjQDKS8Wt2",
        "outputId": "6a9b5567-684a-401b-95b5-8184696a16ac"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version:  3.4.3\n",
            "Apache Spark version:  3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline according to demo as shown below\n",
        "# https://nlp.johnsnowlabs.com/2022/03/31/lemma_csui_id_3_0.html\n",
        "\n",
        "document = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "sentence = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\", \"xx\").setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
        "tokenizer = Tokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\")\n",
        "lemma = LemmatizerModel.pretrained(\"lemma_csui\", \"id\").setInputCols([\"sentence\", \"token\"]).setOutputCol(\"lemma\")\n",
        "pipeline = Pipeline(stages=[document, sentence, tokenizer, lemma])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL1uvD-t8tfH",
        "outputId": "f709ec1d-abb8-455e-dd2d-4d3ffa3936f0"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "lemma_csui download started this may take some time.\n",
            "Approximate size to download 85.5 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(txt):\n",
        "    data = spark.createDataFrame([[txt]]).toDF(\"text\")\n",
        "    result = pipeline.fit(data).transform(data)\n",
        "\n",
        "    lemma_stc = []\n",
        "    for x in result.head().lemma[1:]:\n",
        "        lemma_stc.append(x.result)\n",
        "    return (\" \".join(lemma_stc))"
      ],
      "metadata": {
        "id": "PauGFvtT9ZU7"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_text = \"presiden indonesia joko widodo melakukan pengesahan persetujuan pernyataan pemindahan ibu kota negara\"\n",
        "print(lemmatize(testing_text))"
      ],
      "metadata": {
        "id": "hmNFOEiy9b43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9472ea3-abc4-44c4-b212-bfe039b4dfbc"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "presiden indonesia joko widodo laku pengesahan setuju pernyataan pemindahan ibu kota negara\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining all preprocessing methods"
      ],
      "metadata": {
        "id": "QmSFjtVtwDP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_advanced(text: str,\n",
        "                        freq: Counter,\n",
        "                        threshold: int):\n",
        "    text = remove_noise(text, freq, threshold)\n",
        "    text = normalize_slang(text)\n",
        "    text = remove_stopword(text)\n",
        "    text = lemmatize(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "ca6-_wvbvEyd"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_frequency(data: pd.DataFrame):\n",
        "        # generate word frequency table\n",
        "        word_list = []\n",
        "        for tweet in data['tweet']:\n",
        "            for word in tweet.split():\n",
        "                word_list.append(word)\n",
        "        \n",
        "        return Counter(word_list)"
      ],
      "metadata": {
        "id": "8PyXp9Uyvlgc"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data: pd.DataFrame, threshold: int):\n",
        "    clean = copy.deepcopy(data)\n",
        "    counter = count_frequency(clean)\n",
        "    clean['tweet'] = clean['tweet'].apply(preprocess_basic)\n",
        "    clean['tweet'] = clean['tweet'].apply(preprocess_advanced, args =  (counter, threshold))\n",
        "    return clean"
      ],
      "metadata": {
        "id": "qVpQpbmdtuXm"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# E. Model"
      ],
      "metadata": {
        "id": "nmS6j1B8AB7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = preprocess(df, 5)\n",
        "#dataset_10 = preprocess(df, 10)"
      ],
      "metadata": {
        "id": "cQk5pSVSusFJ"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_w2v_arr(tokenized_x, **kwargs):\n",
        "    w2v_model = Word2Vec(\n",
        "                    tokenized_x, \n",
        "                    size = kwargs['size'], \n",
        "                    min_count = kwargs['min_count'], \n",
        "                    window = kwargs['window'], \n",
        "                    workers = 4, \n",
        "                    sg = 1, \n",
        "                    alpha = 0.01, \n",
        "                    min_alpha = 0.0001, \n",
        "                    seed = 777\n",
        "                )\n",
        "\n",
        "    res = np.zeros((len(tokenized_x), kwargs['size']))\n",
        "    for i in range(len(tokenized_x)):\n",
        "        for word in tokenized_x[i]:\n",
        "            try:\n",
        "                res[i] += w2v_model[word]\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "    if kwargs['scaling'] == 1:\n",
        "        res = MinMaxScaler().fit_transform(res)\n",
        "    return res"
      ],
      "metadata": {
        "id": "_9OGfp0t5yMP"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_d2v_arr(tokenized_x, **kwargs):\n",
        "    d2v_model = Doc2Vec(\n",
        "                    size = kwargs['size'], \n",
        "                    alpha = kwargs['alpha'], \n",
        "                    min_alpha = 0.00025, \n",
        "                    min_count = 1, \n",
        "                    dm = 1\n",
        "                )\n",
        "    d2v_model.build_vocab(tokenized_x)\n",
        "\n",
        "    for epoch in range(kwargs['max_epochs']):\n",
        "        d2v_model.train(tokenized_x, total_examples = d2v_model.corpus_count, epochs=d2v_model.iter)\n",
        "        d2v_model.alpha -= 0.0002 # decrease the learning rate\n",
        "        d2v_model.min_alpha = d2v_model.alpha # fix the learning rate, no decay\n",
        "    \n",
        "    res = np.zeros((len(tokenized_x), kwargs['size']))\n",
        "    for i in range(len(tokenized_x)):\n",
        "        res[i] = d2v_model.docvecs[i]\n",
        "    \n",
        "    if kwargs['scaling'] == 1:\n",
        "        res = MinMaxScaler().fit_transform(x_d2v)\n",
        "    return res"
      ],
      "metadata": {
        "id": "fuovhirn9MEd"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Model:\n",
        "    def __init__(self,\n",
        "                 data: pd.DataFrame,\n",
        "                 split: float,\n",
        "                 feature: str,\n",
        "                 **kwargs):\n",
        "        \n",
        "        # initialize parameter\n",
        "        self.data = data\n",
        "        self.split = split\n",
        "        self.feature = feature\n",
        "        self.kwargs = kwargs\n",
        "        \n",
        "        # separate features and label\n",
        "        self.x = self.data['tweet']\n",
        "        self.y = self.data['sentiment']\n",
        "\n",
        "        # feature extraction\n",
        "        self.x_train, self.x_test, self.y_train, self.y_test = self._create_feature()\n",
        "\n",
        "        # create model\n",
        "        self._create_model()\n",
        "\n",
        "    def _create_feature(self):\n",
        "        lb=LabelEncoder()\n",
        "        self.y = lb.fit_transform(self.y)\n",
        "\n",
        "        if self.feature == \"ttseq\":\n",
        "            data_temp = copy.deepcopy(self.data)\n",
        "            tokenizer = keras.preprocessing.text.Tokenizer(num_words = 100, split = ' ')\n",
        "            tokenizer.fit_on_texts(self.x)\n",
        "\n",
        "            self.x = tokenizer.texts_to_sequences(data_temp['tweet'].values)\n",
        "            self.x = pad_sequences(self.x)\n",
        "\n",
        "            x_train, x_test, y_train, y_test = train_test_split(\n",
        "                                                    self.x, self.y, \n",
        "                                                    test_size = self.split,\n",
        "                                                    random_state = 777, # lucky number\n",
        "                                                    stratify = y\n",
        "                                               )\n",
        "            \n",
        "        elif self.feature == \"w2v\":\n",
        "            w2v_param = {\n",
        "                'size': 100,\n",
        "                'min_count': 2,\n",
        "                'window': 1,\n",
        "                'scaling': 0\n",
        "            }\n",
        "\n",
        "            if 'w2v_param' in self.kwargs:\n",
        "                w2v_param = self.kwargs['w2v_param']\n",
        "\n",
        "            x_token = self.x.apply(word_tokenize)\n",
        "            x_token = x_token.reset_index(drop = True)\n",
        "\n",
        "            self.x = get_w2v_arr(x_token, **w2v_param)\n",
        "\n",
        "            x_train, x_test, y_train, y_test = train_test_split(\n",
        "                                                    self.x, self.y, \n",
        "                                                    test_size = self.split,\n",
        "                                                    random_state = 777, # lucky number\n",
        "                                                    stratify = y\n",
        "                                               )\n",
        "        \n",
        "        elif self.feature == \"d2v\":\n",
        "            d2v_param = {\n",
        "                'size': 100,\n",
        "                'max_epochs': 20,\n",
        "                'alpha': 0.025,\n",
        "                'scaling': 0\n",
        "            }\n",
        "\n",
        "            if 'd2v_param' in self.kwargs:\n",
        "                w2v_param = self.kwargs['w2v_param']\n",
        "            \n",
        "            x_tag_token = [TaggedDocument(words=word_tokenize(w), tags=[str(i)]) for i, w in enumerate(self.x)]\n",
        "            self.x = get_d2v_arr(x_tag_token, **d2v_param)\n",
        "\n",
        "            x_train, x_test, y_train, y_test = train_test_split(\n",
        "                                                    self.x, self.y, \n",
        "                                                    test_size = self.split,\n",
        "                                                    random_state = 777, # lucky number\n",
        "                                                    stratify = y\n",
        "                                               )     \n",
        "\n",
        "        return [x_train, x_test, y_train, y_test]\n",
        "    \n",
        "    def _create_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim = 200, output_dim = self.x.shape[1], input_length = self.x.shape[1]))\n",
        "        model.add(SpatialDropout1D(0.4))\n",
        "        model.add(LSTM(500, dropout = 0.2, recurrent_dropout = 0.2))\n",
        "        model.add(Dense(2, activation = 'softmax'))\n",
        "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "        print(model.summary())\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def train_model(self):\n",
        "        self.model.fit(self.x_train, self.y_train, epochs = 25, batch_size = 64, verbose = 'auto')\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        y_pred = self.model.predict(self.x_train, batch_size = 64, verbose = 'auto')\n",
        "        y_pred = np.argmax(y_pred, axis = 1)\n",
        "        print(\"\\nTraining Dataset\")\n",
        "        print(\"=====================================================\")\n",
        "        print(f\"accuracy score  : {accuracy_score(self.y_train, y_pred) * 100}\")\n",
        "        print(f\"precision score : {precision_score(self.y_train, y_pred, average = 'macro') * 100}\")\n",
        "        print(f\"recall score    : {recall_score(self.y_train, y_pred, average = 'macro') * 100}\")\n",
        "        print(f\"f1-score        : {f1_score(self.y_train, y_pred, average = 'macro') * 100}\")\n",
        "\n",
        "        y_pred = self.model.predict(self.x_test, batch_size = 64, verbose = 'auto')\n",
        "        y_pred = np.argmax(y_pred, axis = 1)\n",
        "        print(\"\\nTesting Dataset\")\n",
        "        print(\"=====================================================\")\n",
        "        print(f\"accuracy score  : {accuracy_score(self.y_test, y_pred) * 100}\")\n",
        "        print(f\"precision score : {precision_score(self.y_test, y_pred, average = 'macro') * 100}\")\n",
        "        print(f\"recall score    : {recall_score(self.y_test, y_pred, average = 'macro') * 100}\")\n",
        "        print(f\"f1-score        : {f1_score(self.y_test, y_pred, average = 'macro') * 100}\")\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "ptg0FzXTzfYA"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM + Text To Sequence"
      ],
      "metadata": {
        "id": "pEj1SaAHHqz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM_Model(dataset, 0.2, \"ttseq\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlZs4_ZZ73M2",
        "outputId": "11017d01-053c-45dc-bbdf-19c6a19f967b"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_38 (Embedding)    (None, 22, 22)            4400      \n",
            "                                                                 \n",
            " spatial_dropout1d_38 (Spati  (None, 22, 22)           0         \n",
            " alDropout1D)                                                    \n",
            "                                                                 \n",
            " lstm_38 (LSTM)              (None, 500)               1046000   \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 2)                 1002      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,051,402\n",
            "Trainable params: 1,051,402\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLcbJNeb81kK",
        "outputId": "a9dc5ea3-d262-4787-ce25-eed591d042b3"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "5/5 [==============================] - 6s 601ms/step - loss: 0.6873 - accuracy: 0.5902\n",
            "Epoch 2/25\n",
            "5/5 [==============================] - 3s 593ms/step - loss: 0.6763 - accuracy: 0.6015\n",
            "Epoch 3/25\n",
            "5/5 [==============================] - 3s 591ms/step - loss: 0.6766 - accuracy: 0.6015\n",
            "Epoch 4/25\n",
            "5/5 [==============================] - 3s 584ms/step - loss: 0.6700 - accuracy: 0.6015\n",
            "Epoch 5/25\n",
            "5/5 [==============================] - 3s 587ms/step - loss: 0.6559 - accuracy: 0.6015\n",
            "Epoch 6/25\n",
            "5/5 [==============================] - 3s 590ms/step - loss: 0.6494 - accuracy: 0.6015\n",
            "Epoch 7/25\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.6357 - accuracy: 0.6654\n",
            "Epoch 8/25\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.6277 - accuracy: 0.6992\n",
            "Epoch 9/25\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.5997 - accuracy: 0.6466\n",
            "Epoch 10/25\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.5661 - accuracy: 0.6617\n",
            "Epoch 11/25\n",
            "5/5 [==============================] - 3s 598ms/step - loss: 0.5676 - accuracy: 0.6992\n",
            "Epoch 12/25\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.5207 - accuracy: 0.7293\n",
            "Epoch 13/25\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.4954 - accuracy: 0.7256\n",
            "Epoch 14/25\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.4740 - accuracy: 0.7632\n",
            "Epoch 15/25\n",
            "5/5 [==============================] - 3s 594ms/step - loss: 0.5009 - accuracy: 0.7556\n",
            "Epoch 16/25\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.5040 - accuracy: 0.7256\n",
            "Epoch 17/25\n",
            "5/5 [==============================] - 3s 587ms/step - loss: 0.4935 - accuracy: 0.7218\n",
            "Epoch 18/25\n",
            "5/5 [==============================] - 3s 592ms/step - loss: 0.4432 - accuracy: 0.7895\n",
            "Epoch 19/25\n",
            "5/5 [==============================] - 3s 596ms/step - loss: 0.4308 - accuracy: 0.7556\n",
            "Epoch 20/25\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.4470 - accuracy: 0.7857\n",
            "Epoch 21/25\n",
            "5/5 [==============================] - 3s 596ms/step - loss: 0.4134 - accuracy: 0.7782\n",
            "Epoch 22/25\n",
            "5/5 [==============================] - 3s 594ms/step - loss: 0.3976 - accuracy: 0.8045\n",
            "Epoch 23/25\n",
            "5/5 [==============================] - 3s 592ms/step - loss: 0.4229 - accuracy: 0.8120\n",
            "Epoch 24/25\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.4024 - accuracy: 0.8045\n",
            "Epoch 25/25\n",
            "5/5 [==============================] - 3s 593ms/step - loss: 0.3930 - accuracy: 0.8158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPIHF3B5Dtm4",
        "outputId": "d64bd3c9-4a38-4bf4-b69c-abbc542329be"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Dataset\n",
            "=====================================================\n",
            "accuracy score  : 83.0827067669173\n",
            "precision score : 82.43089199202052\n",
            "recall score    : 83.5495283018868\n",
            "f1-score        : 82.71105654654438\n",
            "\n",
            "Testing Dataset\n",
            "=====================================================\n",
            "accuracy score  : 79.1044776119403\n",
            "precision score : 78.96613190730838\n",
            "recall score    : 80.0925925925926\n",
            "f1-score        : 78.87387387387388\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM + Word2Vec"
      ],
      "metadata": {
        "id": "JiFsFIUaIOYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM_Model(dataset, 0.2, \"w2v\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTQkjJmfIQyI",
        "outputId": "1ee3ec32-39b1-4918-88d0-4ecbbfda28c8"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_39 (Embedding)    (None, 100, 100)          20000     \n",
            "                                                                 \n",
            " spatial_dropout1d_39 (Spati  (None, 100, 100)         0         \n",
            " alDropout1D)                                                    \n",
            "                                                                 \n",
            " lstm_39 (LSTM)              (None, 500)               1202000   \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 2)                 1002      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,223,002\n",
            "Trainable params: 1,223,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY1E6_FvIYU5",
        "outputId": "5537aeec-c484-462b-9c5f-97e897a2d694"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "5/5 [==============================] - 20s 3s/step - loss: 0.6791 - accuracy: 0.5865\n",
            "Epoch 2/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6723 - accuracy: 0.6015\n",
            "Epoch 3/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6757 - accuracy: 0.6015\n",
            "Epoch 4/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6777 - accuracy: 0.6015\n",
            "Epoch 5/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6736 - accuracy: 0.6015\n",
            "Epoch 6/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6777 - accuracy: 0.6015\n",
            "Epoch 7/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6750 - accuracy: 0.6015\n",
            "Epoch 8/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6749 - accuracy: 0.6015\n",
            "Epoch 9/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6717 - accuracy: 0.6015\n",
            "Epoch 10/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6818 - accuracy: 0.6015\n",
            "Epoch 11/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6729 - accuracy: 0.6015\n",
            "Epoch 12/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6739 - accuracy: 0.6015\n",
            "Epoch 13/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6741 - accuracy: 0.6015\n",
            "Epoch 14/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6739 - accuracy: 0.6015\n",
            "Epoch 15/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6745 - accuracy: 0.6015\n",
            "Epoch 16/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6741 - accuracy: 0.6015\n",
            "Epoch 17/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6727 - accuracy: 0.6015\n",
            "Epoch 18/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6739 - accuracy: 0.6015\n",
            "Epoch 19/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6741 - accuracy: 0.6015\n",
            "Epoch 20/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6803 - accuracy: 0.6015\n",
            "Epoch 21/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6752 - accuracy: 0.6015\n",
            "Epoch 22/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6737 - accuracy: 0.6015\n",
            "Epoch 23/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6756 - accuracy: 0.6015\n",
            "Epoch 24/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6737 - accuracy: 0.6015\n",
            "Epoch 25/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6731 - accuracy: 0.6015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcviSDyZJ4pR",
        "outputId": "871c0388-2bac-4033-98b0-243e0cdea54e"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Dataset\n",
            "=====================================================\n",
            "accuracy score  : 60.150375939849624\n",
            "precision score : 30.075187969924812\n",
            "recall score    : 50.0\n",
            "f1-score        : 37.558685446009385\n",
            "\n",
            "Testing Dataset\n",
            "=====================================================\n",
            "accuracy score  : 59.70149253731343\n",
            "precision score : 29.850746268656714\n",
            "recall score    : 50.0\n",
            "f1-score        : 37.38317757009346\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM + Doc2Vec"
      ],
      "metadata": {
        "id": "oQbgGf-uKVpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM_Model(dataset, 0.2, \"d2v\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pORrJ95vKTlL",
        "outputId": "bdad2c6a-367c-4a0e-aeaa-1755f9ea2bcd"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_40 (Embedding)    (None, 100, 100)          20000     \n",
            "                                                                 \n",
            " spatial_dropout1d_40 (Spati  (None, 100, 100)         0         \n",
            " alDropout1D)                                                    \n",
            "                                                                 \n",
            " lstm_40 (LSTM)              (None, 500)               1202000   \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 2)                 1002      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,223,002\n",
            "Trainable params: 1,223,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnAED-eLKcwU",
        "outputId": "f517e29a-e06f-4864-a309-fd167d1e1cbe"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "5/5 [==============================] - 21s 3s/step - loss: 0.6779 - accuracy: 0.5827\n",
            "Epoch 2/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6789 - accuracy: 0.6015\n",
            "Epoch 3/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6787 - accuracy: 0.6015\n",
            "Epoch 4/25\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.6734 - accuracy: 0.6015\n",
            "Epoch 5/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6771 - accuracy: 0.6015\n",
            "Epoch 6/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6743 - accuracy: 0.6015\n",
            "Epoch 7/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6753 - accuracy: 0.6015\n",
            "Epoch 8/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6749 - accuracy: 0.6015\n",
            "Epoch 9/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6718 - accuracy: 0.6015\n",
            "Epoch 10/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6763 - accuracy: 0.6015\n",
            "Epoch 11/25\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.6742 - accuracy: 0.6015\n",
            "Epoch 12/25\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.6739 - accuracy: 0.6015\n",
            "Epoch 13/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6765 - accuracy: 0.6015\n",
            "Epoch 14/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6725 - accuracy: 0.6015\n",
            "Epoch 15/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6732 - accuracy: 0.6015\n",
            "Epoch 16/25\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.6737 - accuracy: 0.6015\n",
            "Epoch 17/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6695 - accuracy: 0.6015\n",
            "Epoch 18/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6738 - accuracy: 0.6015\n",
            "Epoch 19/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6777 - accuracy: 0.6015\n",
            "Epoch 20/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6755 - accuracy: 0.6015\n",
            "Epoch 21/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6727 - accuracy: 0.6015\n",
            "Epoch 22/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6726 - accuracy: 0.6015\n",
            "Epoch 23/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6766 - accuracy: 0.6015\n",
            "Epoch 24/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6745 - accuracy: 0.6015\n",
            "Epoch 25/25\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.6753 - accuracy: 0.6015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwAGlzmCKenw",
        "outputId": "13389cab-078c-4b06-cb7a-d44c33359d2d"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Dataset\n",
            "=====================================================\n",
            "accuracy score  : 60.150375939849624\n",
            "precision score : 30.075187969924812\n",
            "recall score    : 50.0\n",
            "f1-score        : 37.558685446009385\n",
            "\n",
            "Testing Dataset\n",
            "=====================================================\n",
            "accuracy score  : 59.70149253731343\n",
            "precision score : 29.850746268656714\n",
            "recall score    : 50.0\n",
            "f1-score        : 37.38317757009346\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}